{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LSTM_NODES =256\n",
    "NUM_SENTENCES = 20000\n",
    "MAX_SENTENCE_LENGTH = 50\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To develop such a model, we need a dataset that contains English sentences and their Turkish translations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In our dataset, we do not need to process the input, however, we need to generate two copies of the translated sentence: one with the start-of-sentence token and the other with the end-of-sentence token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples input: 20000\n",
      "num samples output: 20000\n",
      "num samples output input: 20000\n"
     ]
    }
   ],
   "source": [
    "# Each line is split into two substrings at the position where the tab occurs.\n",
    "# The left substring (the English sentence) is inserted into the input_sentences[] list.\n",
    "# The substring to the right of the tab is the corresponding translated Turkish sentence.\n",
    "# The <eos> token, which marks the end-of-sentence is prefixed to the translated sentence,\n",
    "# and the resultant sentence is appended to the output_sentences[] list.\n",
    "# Similarly, the <sos> token, which stands for \"start of sentence\",\n",
    "# is concatenated at the start of the translated sentence and the result is added to the output_sentences_inputs[] list.\n",
    "# The loop terminates if the number of sentences added to the lists is greater than the NUM_SENTENCES variable, i.e. 20,000.\n",
    "\n",
    "input_sentences = []\n",
    "output_sentences = []\n",
    "output_sentences_inputs = []\n",
    "\n",
    "count = 0\n",
    "for line in open(r'tur.txt', encoding=\"utf-8\"):\n",
    "    count += 1\n",
    "\n",
    "    if count > NUM_SENTENCES:\n",
    "        break\n",
    "\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "\n",
    "    input_sentence, output = line.rstrip().split('\\t')\n",
    "\n",
    "    output_sentence = output + ' <eos>'\n",
    "    output_sentence_input = '<sos> ' + output\n",
    "\n",
    "    input_sentences.append(input_sentence)\n",
    "    output_sentences.append(output_sentence)\n",
    "    output_sentences_inputs.append(output_sentence_input)\n",
    "\n",
    "print(\"num samples input:\", len(input_sentences))\n",
    "print(\"num samples output:\", len(output_sentences))\n",
    "print(\"num samples output input:\", len(output_sentences_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm up.\n",
      "Ben uyanığım. <eos>\n",
      "<sos> Ben uyanığım.\n"
     ]
    }
   ],
   "source": [
    "print(input_sentences[47])\n",
    "print(output_sentences[47])\n",
    "print(output_sentences_inputs[47])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenizing the original and translated sentences and applying padding to the sentences that are longer or shorter than a certain length. This is extremely important since deep learning and machine learning algorithms work with numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the input: 3951\n",
      "Length of longest sentence in input: 5\n"
     ]
    }
   ],
   "source": [
    "input_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "input_tokenizer.fit_on_texts(input_sentences)\n",
    "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
    "\n",
    "word2idx_inputs = input_tokenizer.word_index\n",
    "print('Total unique words in the input: %s' % len(word2idx_inputs))\n",
    "\n",
    "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
    "print(\"Length of longest sentence in input: %g\" % max_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the output: 11047\n",
      "Length of longest sentence in the output: 8\n"
     ]
    }
   ],
   "source": [
    "output_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "output_tokenizer.fit_on_texts(output_sentences + output_sentences_inputs)\n",
    "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
    "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
    "\n",
    "word2idx_outputs = output_tokenizer.word_index\n",
    "print('Total unique words in the output: %s' % len(word2idx_outputs))\n",
    "\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
    "print(\"Length of longest sentence in the output: %g\" % max_out_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We need to pad the input. The reason behind padding the input and the output is that text sentences can be of varying length, however LSTM (the algorithm that we are going to train our model) expects input instances with the same length. Therefore, we need to convert our sentences into fixed-length vectors. One way to do this is via padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_sequences.shape: (20000, 5)\n",
      "encoder_input_sequences[47]: [ 0  0  0  7 34]\n"
     ]
    }
   ],
   "source": [
    "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
    "print(\"encoder_input_sequences.shape:\", encoder_input_sequences.shape)\n",
    "print(\"encoder_input_sequences[47]:\", encoder_input_sequences[47])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "print(word2idx_inputs[\"i'm\"])\n",
    "print(word2idx_inputs[\"up\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input_sequences.shape: (20000, 8)\n",
      "decoder_input_sequences[47]: [   2    4 1087    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_input_sequences.shape:\", decoder_input_sequences.shape)\n",
    "print(\"decoder_input_sequences[47]:\", decoder_input_sequences[47])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "1087\n"
     ]
    }
   ],
   "source": [
    "print(word2idx_outputs[\"<sos>\"])\n",
    "print(word2idx_outputs[\"ben\"])\n",
    "print(word2idx_outputs[\"uyanığım.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For English sentences, i.e. the inputs, we will use the GloVe word embeddings. For the translated Turkish sentences in the output, we will use custom word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Glove\n",
    "\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open(r'glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall that we have 3951 unique words in the input.\n",
    "# We will create a matrix where the row number will represent the integer value \n",
    "# for the word and the columns will correspond to the dimensions of the word.\n",
    "# This matrix will contain the word embeddings for the words in our input sentences.\n",
    "\n",
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "embedding_matrix = zeros((num_words, EMBEDDING_SIZE))\n",
    "for word, index in word2idx_inputs.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.21469    0.43367    0.33964   -0.65715    0.15546    0.15318\n",
      " -0.62081    0.27839   -0.3704     0.0029626  0.37131    0.32756\n",
      " -0.32802    0.10206    0.52715   -0.33415   -0.012657   0.20382\n",
      " -0.19846    0.10483    0.72682    0.30136    0.73955    0.2264\n",
      "  0.5213    -0.46339   -0.56209   -0.47684    0.056159  -0.46364\n",
      " -0.18426    0.15954    0.23868   -0.030124  -0.18315    0.27942\n",
      "  0.031251  -0.16198   -0.18941    0.2571    -0.48811   -0.70303\n",
      " -0.0055224 -0.63184   -0.17694    0.38916   -0.64778   -0.08909\n",
      "  0.17655   -1.2462    -0.21257   -0.20355    0.11958    1.6196\n",
      " -0.77112   -2.8367    -0.21148    0.11873    2.1393     0.78805\n",
      "  0.41318    0.97607   -0.67157    0.29821    0.12548    0.10129\n",
      "  0.69104    0.61075    0.58256    0.3346     0.042307  -0.45933\n",
      " -0.24029   -0.73154   -0.3054     0.19878   -0.34562    0.0035721\n",
      " -0.57002    0.027172   0.68865    0.4502    -0.62077    0.36449\n",
      " -1.2001     0.15149    0.58623    0.35867   -0.22877   -0.032302\n",
      " -0.18218    0.18319   -0.34823   -0.36982   -0.61882   -0.38964\n",
      "  0.0028948  0.046601   0.83004    0.40299  ]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_dictionary[\"up\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.21469     0.43367001  0.33963999 -0.65714997  0.15546     0.15318\n",
      " -0.62080997  0.27838999 -0.37040001  0.0029626   0.37131     0.32756001\n",
      " -0.32802001  0.10206     0.52714998 -0.33414999 -0.012657    0.20382001\n",
      " -0.19846     0.10483     0.72681999  0.30136001  0.73954999  0.2264\n",
      "  0.52130002 -0.46338999 -0.56208998 -0.47683999  0.056159   -0.46364\n",
      " -0.18426     0.15954     0.23868001 -0.030124   -0.18314999  0.27941999\n",
      "  0.031251   -0.16198    -0.18941     0.25709999 -0.48811001 -0.70302999\n",
      " -0.0055224  -0.63183999 -0.17693999  0.38916001 -0.64778    -0.08909\n",
      "  0.17655    -1.24619997 -0.21257    -0.20355     0.11958     1.61960006\n",
      " -0.77112001 -2.83669996 -0.21148001  0.11873     2.13930011  0.78805\n",
      "  0.41317999  0.97606999 -0.67157     0.29821     0.12548     0.10129\n",
      "  0.69103998  0.61075002  0.58256     0.3346      0.042307   -0.45932999\n",
      " -0.24029    -0.73154002 -0.30540001  0.19878    -0.34562001  0.0035721\n",
      " -0.57002002  0.027172    0.68865001  0.45019999 -0.62076998  0.36449\n",
      " -1.20009995  0.15149     0.58622998  0.35867    -0.22877    -0.032302\n",
      " -0.18218     0.18319    -0.34823    -0.36982    -0.61882001 -0.38964\n",
      "  0.0028948   0.046601    0.83003998  0.40299001]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix[34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following script creates the embedding layer for the input:\n",
    "embedding_layer = Embedding(num_words, EMBEDDING_SIZE, weights=[embedding_matrix], input_length=max_input_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The first thing we need to do is to define our outputs, as we know that the output will be a sequence of words. Recall that the total number of unique words in the output are 11047. Therefore, each word in the output can be any of the 11047 words. The length of an output sentence is 5. And for each input sentence, we need a corresponding output sentence. Therefore, the final shape of the output will be: (number of inputs, length of the output sentence, the number of words in the output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the empty output array\n",
    "\n",
    "decoder_targets_one_hot = np.zeros((\n",
    "        len(input_sentences),\n",
    "        max_out_len,\n",
    "        num_words_output\n",
    "    ),\n",
    "    dtype='float32'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 8, 11048)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_targets_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next, we need to create the encoder and decoders. The input to the encoder will be the sentence in English and the output will be the hidden state and cell state of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(decoder_output_sequences):\n",
    "    for t, word in enumerate(d):\n",
    "        decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs_placeholder = Input(shape=(max_input_len,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = LSTM(LSTM_NODES, return_state=True)\n",
    "\n",
    "encoder_outputs, h, c = encoder(x)\n",
    "encoder_states = [h, c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The decoder will have two inputs: the hidden state and cell state from the encoder and the input sentence, which actually will be the output sentence with an sos token appended at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs_placeholder = Input(shape=(max_out_len,))\n",
    "\n",
    "decoder_embedding = Embedding(num_words_output, LSTM_NODES)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "decoder_lstm = LSTM(LSTM_NODES, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output from the decoder LSTM is passed through a dense layer to predict decoder outputs.\n",
    "\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "model = Model([encoder_inputs_placeholder,\n",
    "  decoder_inputs_placeholder], decoder_outputs)\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "282/282 [==============================] - 163s 555ms/step - loss: 3.1609 - accuracy: 0.6290 - val_loss: 2.3386 - val_accuracy: 0.7122\n",
      "Epoch 2/20\n",
      "282/282 [==============================] - 140s 497ms/step - loss: 2.0075 - accuracy: 0.7339 - val_loss: 2.1628 - val_accuracy: 0.7316\n",
      "Epoch 3/20\n",
      "282/282 [==============================] - 119s 421ms/step - loss: 1.7473 - accuracy: 0.7661 - val_loss: 1.9857 - val_accuracy: 0.7513\n",
      "Epoch 4/20\n",
      "282/282 [==============================] - 157s 557ms/step - loss: 1.5878 - accuracy: 0.7835 - val_loss: 1.8987 - val_accuracy: 0.7630\n",
      "Epoch 5/20\n",
      "282/282 [==============================] - 127s 449ms/step - loss: 1.4733 - accuracy: 0.7958 - val_loss: 1.8836 - val_accuracy: 0.7647\n",
      "Epoch 6/20\n",
      "282/282 [==============================] - 109s 387ms/step - loss: 1.3942 - accuracy: 0.8051 - val_loss: 1.8437 - val_accuracy: 0.7728\n",
      "Epoch 7/20\n",
      "282/282 [==============================] - 110s 390ms/step - loss: 1.3133 - accuracy: 0.8157 - val_loss: 1.8227 - val_accuracy: 0.7733\n",
      "Epoch 8/20\n",
      "282/282 [==============================] - 130s 460ms/step - loss: 1.2548 - accuracy: 0.8229 - val_loss: 1.8216 - val_accuracy: 0.7788\n",
      "Epoch 9/20\n",
      "282/282 [==============================] - 131s 466ms/step - loss: 1.2140 - accuracy: 0.8296 - val_loss: 1.8253 - val_accuracy: 0.7772\n",
      "Epoch 10/20\n",
      "282/282 [==============================] - 120s 424ms/step - loss: 1.1691 - accuracy: 0.8360 - val_loss: 1.8217 - val_accuracy: 0.7770\n",
      "Epoch 11/20\n",
      "282/282 [==============================] - 115s 407ms/step - loss: 1.1261 - accuracy: 0.8425 - val_loss: 1.8270 - val_accuracy: 0.7749\n",
      "Epoch 12/20\n",
      "282/282 [==============================] - 118s 417ms/step - loss: 1.0988 - accuracy: 0.8475 - val_loss: 1.8350 - val_accuracy: 0.7789\n",
      "Epoch 13/20\n",
      "282/282 [==============================] - 114s 404ms/step - loss: 1.0606 - accuracy: 0.8517 - val_loss: 1.8434 - val_accuracy: 0.7794\n",
      "Epoch 14/20\n",
      "282/282 [==============================] - 124s 439ms/step - loss: 1.0341 - accuracy: 0.8556 - val_loss: 1.8569 - val_accuracy: 0.7768\n",
      "Epoch 15/20\n",
      "282/282 [==============================] - 124s 441ms/step - loss: 1.0058 - accuracy: 0.8617 - val_loss: 1.8479 - val_accuracy: 0.7770\n",
      "Epoch 16/20\n",
      "282/282 [==============================] - 114s 405ms/step - loss: 0.9918 - accuracy: 0.8639 - val_loss: 1.8931 - val_accuracy: 0.7765\n",
      "Epoch 17/20\n",
      "282/282 [==============================] - 112s 398ms/step - loss: 0.9713 - accuracy: 0.8681 - val_loss: 1.9254 - val_accuracy: 0.7733\n",
      "Epoch 18/20\n",
      "282/282 [==============================] - 118s 418ms/step - loss: 0.9576 - accuracy: 0.8724 - val_loss: 1.9274 - val_accuracy: 0.7744\n",
      "Epoch 19/20\n",
      "282/282 [==============================] - 145s 515ms/step - loss: 0.9422 - accuracy: 0.8746 - val_loss: 1.9333 - val_accuracy: 0.7732\n",
      "Epoch 20/20\n",
      "282/282 [==============================] - 114s 403ms/step - loss: 0.9298 - accuracy: 0.8779 - val_loss: 1.9667 - val_accuracy: 0.7736\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "r = model.fit(\n",
    "    [encoder_input_sequences, decoder_input_sequences],\n",
    "    decoder_targets_one_hot,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs_placeholder, encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Since now at each step we need the decoder hidden and cell states, we will modify our model to accept the hidden and cell states as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(LSTM_NODES,))\n",
    "decoder_state_input_c = Input(shape=(LSTM_NODES,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now at each time step, there will be only single word in the decoder input, we need to modify the decoder embedding layer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next, we need to create the placeholder for decoder outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To make predictions, the decoder output is passed through the dense layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_states = [h, c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The final step is to define the updated decoder model, as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model = Model(\n",
    "    [decoder_inputs_single] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the tokenization steps, we converted words to integers. The outputs from the decoder will also be integers. However, we want our output to be a sequence of words in the Turkish language. To do so, we need to convert the integers back to words. We will create new dictionaries for both inputs and outputs where the keys will be the integers and the corresponding values will be the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word_input = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_target = {v:k for k, v in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The method will accept an input-padded sequence English sentence (in the integer form) and will return the translated Turkish sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = word2idx_outputs['<sos>']\n",
    "    eos = word2idx_outputs['<eos>']\n",
    "    output_sentence = []\n",
    "\n",
    "    for _ in range(max_out_len):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "\n",
    "        if idx > 0:\n",
    "            word = idx2word_target[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "        target_seq[0, 0] = idx\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input: I'll be inside.\n",
      "Response: i̇çeri olacağım.\n"
     ]
    }
   ],
   "source": [
    "i = np.random.choice(len(input_sentences))\n",
    "input_seq = encoder_input_sequences[i:i+1]\n",
    "translation = translate_sentence(input_seq)\n",
    "print('-')\n",
    "print('Input:', input_sentences[i])\n",
    "print('Response:', translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
